{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d83fc8c-990a-40c3-9d7c-32765dfda85d",
   "metadata": {},
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbdd1d-a78d-4ad2-9f75-d4c31820169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#virtual environment\n",
    "'''\n",
    "conda activate py310\n",
    "jupyter notebook\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f6979-186a-4db7-a929-6e4b3ffb2ff2",
   "metadata": {},
   "source": [
    "## Training a Tokenizer\n",
    "suppose sequence temperature input data are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80448115-bc89-4fd1-99d3-86c867460666",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = 'KantaiBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96044993-7637-44ad-b20c-8bf85f0befa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text files in current directory: ['../data/temperature_mask/temperature_mask.txt', '../data/temperature_mask/uniprot_sprot_mask.txt']\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 1h 43min 14s, sys: 3min 10s, total: 1h 46min 24s\n",
      "Wall time: 14min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(f\"../data/temperature_mask/*.txt\")]\n",
    "print('text files in current directory:', paths)\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\",]\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e33bf04-bbdf-415e-9d5f-ed733cb71d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " 'IRV',\n",
       " 'LT',\n",
       " 'TYN',\n",
       " 'KT',\n",
       " 'GYF',\n",
       " 'IH',\n",
       " 'KGV',\n",
       " 'QR',\n",
       " 'GVT',\n",
       " 'YDAF',\n",
       " 'IQVE',\n",
       " 'KRLN',\n",
       " 'Ġis',\n",
       " 'Ġ22']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tokenizer\n",
    "example_seq = \"RTIRVLTTYNKTGYFIHKGVQRGVTYDAFIQVEKRLN is 22\"\n",
    "tokenizer.encode(example_seq).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21118ea8-4b37-4d33-b21c-9f415790a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start and end tokens to fit the BERT model\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8203b1bf-966f-49fe-a9b8-1dea087ff491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=17, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(example_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e7e355-e6a1-407b-801d-1cbb2fb4bc0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'RT',\n",
       " 'IRV',\n",
       " 'LT',\n",
       " 'TYN',\n",
       " 'KT',\n",
       " 'GYF',\n",
       " 'IH',\n",
       " 'KGV',\n",
       " 'QR',\n",
       " 'GVT',\n",
       " 'YDAF',\n",
       " 'IQVE',\n",
       " 'KRLN',\n",
       " 'Ġis',\n",
       " 'Ġ22',\n",
       " '</s>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(example_seq).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ef36dcb-009d-4302-ac3c-6dd6cf9657e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=52000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93111472-1b56-445c-b92d-701f92fb0f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip creation of ../KantaiBERT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../KantaiBERT/vocab.json', '../KantaiBERT/merges.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tokenizer\n",
    "import os\n",
    "\n",
    "#Usually, the dir temperature_mask should be created by prepare_data.ipynb previously\n",
    "token_dir = f\"../{output_name}\"\n",
    "if not os.path.isdir(token_dir):\n",
    "  os.makedirs(token_dir)\n",
    "else:\n",
    "    print(f'skip creation of {token_dir}')\n",
    "\n",
    "# save object tokenizer as vocab.json and merges.txt\n",
    "tokenizer.save_model(token_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
